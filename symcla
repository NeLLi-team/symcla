#!/usr/bin/env python


import time
import glob
import os
import re
import subprocess
import pandas as pd
import tarfile
import typer
import shap
import shutil
import json
import xgboost as xgb
from pathlib import Path

"""
symcla: Symbiont Classifier

Author: Juan C. Villada
Email: jvillada@lbl.gov

New Lineages of Life (NeLLi) Group
US Department of Energy Joint Genome Institute (JGI)
Lawrence Berkeley National Laboratory (LBNL)
2024


Usage:
./symcla --help
./symcla setup
./symcla classify --genomedir data/test_genomes --savedir test_output --ncpus 32
"""


def greetings():
    my_name = "\n\nsymcla: symbiont classifier"
    typer.secho(my_name, fg=typer.colors.GREEN, bold=True)


def init_message_setup() -> None:
    message = "\n" + "-" * 10 + " Setup data workflow " + "-" * 10 + "\n"
    message = typer.style(text=message, fg=typer.colors.BRIGHT_GREEN, bold=False)
    return typer.echo(message)


def init_message_build() -> None:
    message = "Building classifiers workflow\n"
    message = typer.style(text=message, fg=typer.colors.BRIGHT_GREEN, bold=False)
    return typer.echo(message)


def init_message_classify() -> None:
    message = "\n" + "-" * 10 + " Classifying genomes workflow " + "-" * 10 + "\n"
    message = typer.style(text=message, fg=typer.colors.BRIGHT_GREEN, bold=False)
    return typer.echo(message)


def extract_data() -> None:
    typer.secho("Extracting required data", fg=typer.colors.BRIGHT_GREEN)
    data_tar = tarfile.open(f"{os.path.dirname(os.path.abspath(__file__))}/data_v1.tar.gz")
    data_tar.extractall(".")
    data_tar.close()
    typer.secho("[OK] Data extracted\n", fg=typer.colors.BRIGHT_MAGENTA)


def create_output_dir(savedir: str) -> None:
    typer.secho("Creating output dir", fg=typer.colors.BRIGHT_GREEN)
    if os.path.exists(savedir):
        typer.secho(
            "[Error] Output directory already exists",
            fg=typer.colors.BRIGHT_RED,
            err=True,
        )
        exit()
    else:
        os.mkdir(savedir)
        typer.secho(
            f"[OK] Output directory created at: {savedir} \n",
            fg=typer.colors.BRIGHT_MAGENTA,
        )


def create_tmp_dir(tmp_dir_path: str) -> None:
    if os.path.exists(tmp_dir_path):
        typer.secho(
            "[Error] Output tmp directory already exists",
            fg=typer.colors.BRIGHT_RED,
            err=True,
        )
        exit()
    else:
        os.mkdir(tmp_dir_path)


def copy_genomes_to_tmp_dir(genomedir: str) -> str:
    tmp_genome_dir_path = f"{tmp_dir_path}/renamed_genomes"
    os.mkdir(tmp_genome_dir_path)
    for each_file in glob.glob(genomedir + "/*.faa"):
        shutil.copy(each_file, tmp_genome_dir_path)
    return tmp_genome_dir_path


def rename_genomes(tmp_genome_dir_path: str) -> str:
    genome_file_paths = glob.glob(tmp_genome_dir_path + "/*.faa")
    genome_names = [x.split("/")[-1].split(".faa")[0] for x in genome_file_paths]
    # the dictionary will have as keys the new simplified name of the genome and as values the original name
    # the simplied name will be genome_1, genome_2, etc
    genome_dict = {f"genome_{i+1}": genome_names[i] for i in range(len(genome_names))}

    genome_dict_path = f"{tmp_dir_path}/genomes_dict.json"
    with open(genome_dict_path, "w") as outfile:
        json.dump(genome_dict, outfile)

    # rename the faa fasta files with the new simplified names using the genome_dict
    for each_genome in genome_file_paths:
        genome_name = each_genome.split("/")[-1].split(".faa")[0]
        new_name = f"{tmp_genome_dir_path}/{list(genome_dict.keys())[list(genome_dict.values()).index(genome_name)]}.faa"
        os.rename(each_genome, new_name)

    typer.secho("[OK] Genomes renamed\n", fg=typer.colors.BRIGHT_MAGENTA)

    return genome_dict_path


def rename_all_proteins_in_fasta_files(tmp_genome_dir_path: str, savedir: str) -> None:
    genome_file_paths = glob.glob(tmp_genome_dir_path + "/*.faa")
    for each_genome in genome_file_paths:
        # create a json dictionary with the original protein names and the new simplified names
        # the simplified names will be protein_1, protein_2, etc
        with open(each_genome, "r") as infile:
            proteins = [line for line in infile if line.startswith(">")]
            proteins = [x.split()[0].replace(">", "") for x in proteins]
            proteins_dict = {
                f"protein_{i+1}": proteins[i] for i in range(len(proteins))
            }
        with open(f"{each_genome.split('.faa')[0]}_dict.json", "w") as outfile:
            json.dump(proteins_dict, outfile)

        # rename the proteins in the fasta file
        with open(each_genome, "r") as infile:
            genome_name = each_genome.split("/")[-1].replace(".faa", "")
            with open(f"{each_genome.replace(".faa", "_renamed.faa")}", "w") as outfile:
                for line in infile:
                    if line.startswith(">"):
                        new_name = f">{genome_name}|protein_{proteins.index(line.split()[0].replace(">", ""))+1}\n"
                        outfile.write(new_name)
                    else:
                        outfile.write(line)
        # delete the original fasta file
        os.remove(each_genome)

        # rename the new fasta file with the original name
        os.rename(f"{each_genome.replace('.faa', '_renamed.faa')}", each_genome)

    typer.secho("[OK] Proteins renamed\n", fg=typer.colors.BRIGHT_MAGENTA)


def merge_genomes(tmp_genomes_path) -> None:
    typer.secho("Merging genomes", fg=typer.colors.BRIGHT_GREEN)
    genome_file_paths = glob.glob(f"{tmp_genomes_path}/*.faa")
    output_file = f"{tmp_dir_path}/merged_genomes.faa"
    with open(output_file, "w") as outfile:
        for each_file in genome_file_paths:
            with open(each_file) as infile:
                for line in infile:
                    outfile.write(line)
    typer.secho(
        f"[OK] Fasta files merged at: {output_file} \n", fg=typer.colors.BRIGHT_MAGENTA
    )


def run_hmmsearch(symcla_model: str, ncpus: int) -> None:
    typer.secho(
        "Running hmmsearch on merged genomes for the hmm models file:\n",
        fg=typer.colors.BRIGHT_GREEN,
    )

    script_path = Path(__file__)
    script_dir = script_path.parent
    data_folder = Path(script_dir) / "data"

    cmd_search = [
        "hmmsearch",
        "--cpu",
        str(ncpus),
        "--tblout",
        tmp_dir_path + "/" + f"{symcla_model}_hmmsearch.tblout",
        "--noali",
        "-E",
        str(1000),
        "--incE",
        str(1000),
        # "-Z", str(61295632),
        data_folder / f"hmms_symcla/{symcla_model}.hmm",
        # f"data/hmms_symcla/{symcla_model}.hmm",
        tmp_dir_path + "/merged_genomes.faa",
        ]

    subprocess.run(cmd_search, stdout=subprocess.PIPE)

    typer.secho(
        f"[OK] Hmmsearch output saved at: {tmp_dir_path}/{symcla_model}_hmmsearch.tblout \n",
        fg=typer.colors.BRIGHT_MAGENTA,
    )


def run_hmmsearch_uni56(ncpus: int) -> None:
    hmmfile = f"{os.path.dirname(os.path.abspath(__file__))}/data/hmms_uni56/uni56.hmm"
    cmd_search = [
        "hmmsearch",
        "--cpu",
        str(ncpus),
        "--tblout",
        tmp_dir_path + "/" + "uni56_hmmsearch.tblout",
        "--noali",
        "--cut_ga",
        hmmfile,
        tmp_dir_path + "/merged_genomes.faa",
        ]
    subprocess.run(cmd_search, stdout=subprocess.PIPE)

    typer.secho(
        f"[OK] Hmmsearch output saved at: {tmp_dir_path}/uni56_hmmsearch.tblout \n",
        fg=typer.colors.BRIGHT_MAGENTA,
    )


def save_list_of_models(symcla_model: str) -> None:
    typer.secho(
        "Saving list of models for each hmm model file", fg=typer.colors.BRIGHT_GREEN
    )

    list_of_hmm_models_files = [f"{os.path.dirname(os.path.abspath(__file__))}/data/hmms_symcla/{symcla_model}.hmm"] + [
        f"{os.path.dirname(os.path.abspath(__file__))}/data/hmms_uni56/uni56.hmm"
    ]

    for hmmfile in list_of_hmm_models_files:
        models_list_file_path = (
                tmp_dir_path + "/" + hmmfile.split("/")[-1].split(".")[0] + "_models.list"
        )
        with open(models_list_file_path, "w") as output_models_list_file:
            with open(hmmfile, "r") as my_hmmfile:
                for line in my_hmmfile:
                    if re.search("NAME", line):
                        output_models_list_file.write(line.replace("NAME  ", ""))

    typer.secho("[OK] Models list saved\n", fg=typer.colors.BRIGHT_MAGENTA)


def save_list_of_genomes(tmp_genomes_path: str) -> None:
    typer.secho("Saving list of genomes", fg=typer.colors.BRIGHT_GREEN)

    list_of_genome_files = glob.glob(f"{tmp_genomes_path}/genome_*.faa")

    genomes_list_file_path = tmp_dir_path + "/genomes.list"

    with open(genomes_list_file_path, "w") as output_genomes_list_file:
        for each_element in list_of_genome_files:
            output_genomes_list_file.write(
                each_element.split("/")[-1].split(".faa")[0] + "\n"
            )

    typer.secho("[OK] Genomes list saved\n", fg=typer.colors.BRIGHT_MAGENTA)


def hmmer_results_to_pandas_df() -> None:
    typer.secho(
        "Generating matrix of highest scores from the hmmsearch output",
        fg=typer.colors.BRIGHT_GREEN,
    )

    list_of_tblout_hmmsearch_output_files = glob.glob(
        tmp_dir_path + "/*_hmmsearch.tblout"
    )

    for tbloutfile in list_of_tblout_hmmsearch_output_files:
        try:
            tblout_hmm_result = pd.read_csv(
                tbloutfile, header=None, sep=r"\s+", comment="#", usecols=[0, 2, 4, 5]
            )
        except Exception as e:
            tblout_hmm_result = pd.DataFrame(columns=[0, 2, 4, 5])


        models_names = pd.read_csv(
            tbloutfile.replace("_hmmsearch.tblout", "_models.list"),
            sep="\t",
            header=None,
        )

        genomes_names = pd.read_csv(
            tmp_dir_path + "/genomes.list", sep="\t", header=None
        )

        # Process hmmsearch output
        tblout_hmm_result = tblout_hmm_result.rename(
            columns={0: "taxon_oid", 2: "model", 4: "evalue", 5: "score"}
        )

        tblout_hmm_result["protein_name"] = tblout_hmm_result["taxon_oid"].str.replace(
            ".*\\|", "", regex=True
        )
        tblout_hmm_result["taxon_oid"] = tblout_hmm_result["taxon_oid"].str.replace(
            "\\|.*$", "", regex=True
        )

        # Grouping by COG and keeping only the max score
        tblout_hmm_result = (
            tblout_hmm_result.groupby(["taxon_oid", "model"]).max().reset_index()
        )

        df_hits_with_protein_names_loc = tbloutfile.replace(
            "_hmmsearch.tblout", "_hits_with_protein_names.tsv"
        )

        global sym50_hits_with_protein_names_loc
        # get the dir to tbloutfile and add the sy50_hits_with_protein_names.tsv
        sym50_hits_with_protein_names_loc = os.path.join(
            os.path.dirname(tbloutfile), "sy50_hits_with_protein_names.tsv"
        )

        tblout_hmm_result.to_csv(
            df_hits_with_protein_names_loc,
            index=False,
            sep="\t",
        )

        tblout_hmm_result = tblout_hmm_result[["taxon_oid", "model", "score"]]
        tblout_hmm_result = tblout_hmm_result.sort_values(
            by=["taxon_oid", "model", "score"]
        ).reset_index(drop=True)
        tblout_hmm_result = tblout_hmm_result.drop_duplicates()
        tblout_hmm_result = tblout_hmm_result.pivot_table(
            index="taxon_oid", columns="model", values="score", fill_value=0
        )

        # Processing all taxa names
        genomes_names = genomes_names.rename(columns={0: "taxon_oid"})
        genomes_names = genomes_names.sort_values(by=["taxon_oid"])

        # Merge dataframes based on taxa names
        tblout_hmm_result = pd.merge(
            left=genomes_names, right=tblout_hmm_result, on="taxon_oid", how="left"
        )

        # Fill missing values for which hmmsearch did not find any result even with the large E-value thresholds
        # This missing values are normally due to sequences being of such low quality that they do not pass the hmmsearch filters
        tblout_hmm_result.fillna(float(0.0), inplace=True)

        # Load all model names to complete matrix with all COG models analysed
        models_names = models_names.rename(columns={0: "model"})
        missing_models = set(models_names["model"]) - set(
            tblout_hmm_result.drop(["taxon_oid"], axis=1).columns
        )
        if len(missing_models) > 0:
            for xmodel in missing_models:
                # tblout_hmm_result[xmodel] = float(0)
                new_column = pd.DataFrame(
                    float(0.0),
                    index=range(len(tblout_hmm_result.index)),
                    columns=[xmodel],
                )
                tblout_hmm_result = pd.concat([tblout_hmm_result, new_column], axis=1)

        # Sort columns alphabetically, except for taxon_oid column which should be the first one
        tblout_hmm_result = tblout_hmm_result.reindex(
            sorted(tblout_hmm_result.columns), axis=1
        )
        tblout_hmm_result = tblout_hmm_result[
            ["taxon_oid"]
            + [col for col in tblout_hmm_result.columns if col != "taxon_oid"]
            ]
        # Save output
        tblout_hmm_result.to_csv(
            tbloutfile.replace("_hmmsearch.tblout", "_hits_all_models.tsv"),
            index=False,
            sep="\t",
        )

    typer.secho(
        "[OK] All 'hits_all_models' tables saved\n", fg=typer.colors.BRIGHT_MAGENTA
    )


def classify_genomes(symcla_model: str, savedir: str) -> None:
    typer.secho(
        f"Classifying genomes using the {symcla_model} model",
        fg=typer.colors.BRIGHT_GREEN,
    )

    all_tblout_df = pd.read_csv(
        tmp_dir_path + f"/{symcla_model}_hits_all_models.tsv",
        sep="\t",
        low_memory=False,
        )
    all_tblout_df.to_csv(f"{savedir}/bitscore.tsv", sep="\t", index=False)

    features_gt0 = all_tblout_df.drop(["taxon_oid"], axis=1).apply(
        lambda x: x[x > 0].count(), axis=1
    )
    features_ge20 = all_tblout_df.drop(["taxon_oid"], axis=1).apply(
        lambda x: x[x >= 20].count(), axis=1
    )
    features_ge100 = all_tblout_df.drop(["taxon_oid"], axis=1).apply(
        lambda x: x[x >= 100].count(), axis=1
    )
    tblout_hmm_result_total_models = pd.concat(
        [all_tblout_df["taxon_oid"], features_gt0, features_ge20, features_ge100],
        axis=1,
    )
    tblout_hmm_result_total_models.columns = [
        "taxon_oid",
        "features_gt0",
        "features_ge20",
        "features_ge100",
    ]
    tblout_hmm_result_total_models.to_csv(
        tmp_dir_path + "/total_models_per_genome.tsv", sep="\t", index=False
    )

    taxon_oid_list = all_tblout_df["taxon_oid"].tolist()

    all_tblout_df.drop(["taxon_oid"], axis=1, inplace=True)
    # all_tblout_df=np.array(all_tblout_df)

    xgb_model = xgb.XGBRegressor()
    xgb_model.load_model(f"{os.path.dirname(os.path.abspath(__file__))}/data/ml_models/{symcla_model}.json")
    symcla_prediction = xgb_model.predict(all_tblout_df)
    predictions_df = pd.DataFrame({"taxon_oid": taxon_oid_list})
    predictions_df["symcla_score"] = symcla_prediction
    predictions_df.to_csv(
        tmp_dir_path + "/symcla_predictions.tsv", sep="\t", index=False
    )

    # SHAP
    typer.secho("Computing SHAP values", fg=typer.colors.BRIGHT_GREEN)
    explainer = shap.Explainer(xgb_model)
    shap_values = explainer(all_tblout_df)
    shap_df = pd.DataFrame(shap_values.values, columns=all_tblout_df.columns)
    shap_df["taxon_oid"] = taxon_oid_list
    shap_df = shap_df[
        ["taxon_oid"] + [col for col in shap_df.columns if col != "taxon_oid"]
        ]
    shap_df.to_csv(f"{savedir}/shap.tsv", sep="\t", index=False)
    typer.secho("[OK] Genomes classified\n", fg=typer.colors.BRIGHT_MAGENTA)


def compute_feature_contribution(savedir: str) -> None:
    typer.secho("Computing feature contribution", fg=typer.colors.BRIGHT_GREEN)

    bitscore_df = pd.read_csv(f"{savedir}/bitscore.tsv", sep="\t", low_memory=False)
    shap_df = pd.read_csv(f"{savedir}/shap.tsv", sep="\t", low_memory=False)
    protein_names_df = pd.read_csv(
        sym50_hits_with_protein_names_loc,
        sep="\t",
    )
    # keep only the columns taxon_oid	model	protein_name
    protein_names_df = protein_names_df[["taxon_oid", "model", "protein_name"]]
    # rename model to feature
    protein_names_df = protein_names_df.rename(columns={"model": "feature"})

    bitscore_melt = bitscore_df.melt(
        id_vars="taxon_oid", var_name="feature", value_name="bitscore"
    )
    shap_melt = shap_df.melt(id_vars="taxon_oid", var_name="feature", value_name="shap")

    bitscore_melt.to_csv(f"{savedir}/bitscore_melt.tsv", sep="\t", index=False)
    shap_melt.to_csv(f"{savedir}/shap_melt.tsv", sep="\t", index=False)

    contribution_df = shap_melt.merge(
        bitscore_melt, on=["taxon_oid", "feature"], how="left"
    )

    features_annotation_df = pd.read_csv(
        f"{os.path.dirname(os.path.abspath(__file__))}/data/feature_annotation_majority_for_symcla.tsv", sep="\t"
    )

    contribution_df = contribution_df.merge(
        features_annotation_df, on="feature", how="left"
    )
    contribution_df = contribution_df.merge(
        protein_names_df, on=["taxon_oid", "feature"], how="left"
    )

    contribution_df = contribution_df[contribution_df["shap"].abs() > 0.01]

    # rename the protein names back to the original names
    genome_dict_path = f"{tmp_dir_path}/genomes_dict.json"
    with open(genome_dict_path, "r") as infile:
        genome_dict = json.load(infile)

    contribution_df["genome_name_renamed"] = contribution_df["taxon_oid"]
    contribution_df["taxon_oid"] = contribution_df["taxon_oid"].replace(genome_dict)

    # create a dictionary of dictionaries with the protein names for each genome
    protein_dict_paths_list = glob.glob(
        f"{tmp_dir_path}/renamed_genomes/genome_*_dict.json"
    )

    # protein_dict_paths will have genome name as key and the path to the protein dictionary as value
    protein_dict_paths = {
        x.split("/")[-1].split("_dict.json")[0]: x for x in protein_dict_paths_list
    }

    # create a dictionary of dictionaries with the protein names for each genome
    protein_dict = {}
    for genome_name, protein_dict_path in protein_dict_paths.items():
        with open(protein_dict_path, "r") as infile:
            protein_dict[genome_name] = json.load(infile)

    contribution_df["protein_name_renamed"] = contribution_df["protein_name"]

    contribution_df["protein_name"] = contribution_df.apply(
        lambda x: protein_dict[x["genome_name_renamed"]].get(
            x["protein_name_renamed"], "absent"
        ),
        axis=1,
    )

    # remove the columns genome_name_renamed and protein_name_renamed
    # contribution_df.drop(
    #     columns=["genome_name_renamed", "protein_name_renamed"], inplace=True
    # )

    contribution_df.to_csv(f"{savedir}/feature_contribution.tsv", sep="\t", index=False)

    # remove shap and bitscore melt files
    os.remove(f"{savedir}/bitscore.tsv")
    os.remove(f"{savedir}/shap.tsv")
    os.remove(f"{savedir}/bitscore_melt.tsv")
    os.remove(f"{savedir}/shap_melt.tsv")

    typer.secho("[OK] Feature contribution computed\n", fg=typer.colors.BRIGHT_MAGENTA)


def count_uni56() -> None:
    df_uni56 = pd.read_csv(tmp_dir_path + "/uni56_hits_all_models.tsv", sep="\t")
    df_uni56 = df_uni56.set_index("taxon_oid", drop=True)
    df_uni56[df_uni56 > 0] = 1
    df_uni56["total_UNI56"] = df_uni56.sum(axis=1)
    df_uni56["completeness_UNI56"] = 100 * (df_uni56["total_UNI56"] / 56)
    df_uni56.to_csv(tmp_dir_path + "/uni56_presence.tsv", sep="\t", index=True)


def save_final_results(savedir: str) -> None:
    classifier_results = pd.read_csv(tmp_dir_path + "/symcla_predictions.tsv", sep="\t")
    uni56_results = pd.read_csv(
        tmp_dir_path + "/uni56_presence.tsv",
        sep="\t",
        usecols=["taxon_oid", "completeness_UNI56"],
        )
    total_models_results = pd.read_csv(
        tmp_dir_path + "/total_models_per_genome.tsv", sep="\t"
    )

    final_df = pd.merge(
        left=classifier_results, right=uni56_results, on=["taxon_oid"], how="inner"
    )

    final_df = pd.merge(
        left=final_df, right=total_models_results, on=["taxon_oid"], how="inner"
    )

    # Reorder columns of final_df dataframe as follows: taxon_oid, completeness_UNI56, total_models, symcla_score
    final_df = final_df[
        [
            "taxon_oid",
            "completeness_UNI56",
            "features_gt0",
            "features_ge20",
            "features_ge100",
            "symcla_score",
        ]
    ]

    # rename the genome names back to the original names
    genome_dict_path = f"{tmp_dir_path}/genomes_dict.json"
    with open(genome_dict_path, "r") as infile:
        genome_dict = json.load(infile)

    final_df["taxon_oid"] = final_df["taxon_oid"].replace(genome_dict)

    round(final_df, 3).to_csv(savedir + "/symcla_results.tsv", sep="\t", index=False)
    typer.secho(
        message="---------- Rounded (3 digits) summary:", fg=typer.colors.BRIGHT_MAGENTA
    )
    typer.secho(message=round(final_df, 3), fg=typer.colors.BRIGHT_BLUE)
    typer.secho(message="\n\n----- DONE -----\n\n", fg=typer.colors.BRIGHT_GREEN)


def remove_temp_files() -> None:
    typer.secho(message="Removing tmp folder\n", fg=typer.colors.BRIGHT_GREEN)
    shutil.rmtree(tmp_dir_path)


app = typer.Typer()
# app = typer.Typer(help=greetings())


@app.command()
def setup():
    init_message_setup()
    greetings()
    extract_data()


@app.command()
def build():
    greetings()
    init_message_build()


@app.command()
def classify(
        genomedir: str = "input_genomes",
        savedir: str = "output_symcla",
        ncpus: int = 16,
        deltmp: bool = True,
):
    symcla_model: str = "sy50"

    # init timer
    start_time = time.time()
    # run pipeline
    greetings()
    init_message_classify()
    savedir=os.path.abspath(savedir)
    genomedir=os.path.abspath(genomedir)
    create_output_dir(savedir=savedir)

    global tmp_dir_path
    tmp_dir_path = f"{savedir}/tmp"
    create_tmp_dir(tmp_dir_path)

    tmp_genomes_path = copy_genomes_to_tmp_dir(genomedir=genomedir)

    rename_genomes(tmp_genome_dir_path=tmp_genomes_path)

    rename_all_proteins_in_fasta_files(
        tmp_genome_dir_path=tmp_genomes_path, savedir=savedir
    )

    merge_genomes(tmp_genomes_path)
    run_hmmsearch(symcla_model=symcla_model, ncpus=ncpus)
    run_hmmsearch_uni56(ncpus=ncpus)
    save_list_of_models(symcla_model=symcla_model)
    save_list_of_genomes(tmp_genomes_path=tmp_genomes_path)
    hmmer_results_to_pandas_df()
    classify_genomes(symcla_model=symcla_model, savedir=savedir)
    compute_feature_contribution(savedir=savedir)
    count_uni56()
    save_final_results(savedir=savedir)
    if deltmp:
        remove_temp_files()

    # end timer
    end_time = time.time()
    typer.secho(
        message=f"Total time: {round(end_time - start_time)} seconds ({round((end_time - start_time)/60, 1)} minutes)",
        fg=typer.colors.BRIGHT_MAGENTA,
    )


if __name__ == "__main__":
    app()
